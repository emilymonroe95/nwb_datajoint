{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NWB-Datajoint Spike Sorting Tutorial\n",
    "\n",
    "**Note: make a copy of this notebook and run the copy to avoid git conflicts in the future**\n",
    "\n",
    "This is the second in a multi-part tutorial on the NWB-Datajoint pipeline used in Loren Frank's lab, UCSF. It demonstrates how to run spike sorting and curate units within the pipeline.\n",
    "\n",
    "If you have not done [tutorial 0](0_intro.ipynb) yet, make sure to do so before proceeding.\n",
    "\n",
    "Let's start by importing the `nwb_datajoint` package, along with a few others.<br>\n",
    "**Note 2: Make sure you are running this within the nwb_datajoint Conda environment)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import datajoint as dj\n",
    "import nwb_datajoint as nd\n",
    "\n",
    "# ignore datajoint+jupyter async warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=ResourceWarning)\n",
    "os.environ['NWB_DATAJOINT_TEMP_DIR']=\"/stelmo/nwb/tmp\"\n",
    "os.environ['KACHERY_STORAGE_DIR']=\"/stelmo/nwb/kachery-storage\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import tables from nwb_datajoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nwb_datajoint.common import (SortGroup, SpikeSortingFilterParameters, SpikeSortingArtifactDetectionParameters,\n",
    "                                  SpikeSortingRecordingSelection, SpikeSortingRecording, \n",
    "                                  SpikeSortingWorkspace, \n",
    "                                  SpikeSorter, SpikeSorterParameters, SortingID,\n",
    "                                  SpikeSortingSelection, SpikeSorting, \n",
    "                                  SpikeSortingMetricParameters,\n",
    "                                  AutomaticCurationParameters, AutomaticCurationSelection,\n",
    "                                  AutomaticCuration,\n",
    "                                  CuratedSpikeSortingSelection, CuratedSpikeSorting,\n",
    "                                  IntervalList, SortInterval, Raw,\n",
    "                                  Lab, LabMember, LabTeam, Session,\n",
    "                                  Nwbfile, AnalysisNwbfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first make sure that you're a part of the LorenLab `LabTeam`, so you'll have the right permissions for this tutorial.<br>Replace `your_name`, `your_email`, and `datajoint_username`, with your information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_name = 'Daniel Gramling'\n",
    "your_email = 'gmail@gmail.com'\n",
    "datajoint_username = 'user'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_member_list = np.unique(LabTeam.LabTeamMember().fetch('lab_member_name')).tolist()\n",
    "lorenlab_team_members = (LabTeam().LabTeamMember() & {'team_name' : 'LorenLab'}).fetch('lab_member_name').tolist()\n",
    "if your_name not in lab_member_list:\n",
    "    LabMember().insert1([your_name, your_name.split()[0], your_name.split()[1]], skip_duplicates=True)\n",
    "    LabMember.LabMemberInfo.insert([your_name, your_email, datajoint_username])\n",
    "    LabTeam.LabTeamMember.insert1({'team_name' : 'LorenLab', \n",
    "                                   'lab_member_name' : your_name}, skip_duplicates=True)\n",
    "    print(f'Hi {your_name}! You have just been added to the LabMember table and the LorenLab team. Congrats!')\n",
    "elif your_name not in lorenlab_team_members:\n",
    "    LabTeam.LabTeamMember.insert1({'team_name' : 'LorenLab', \n",
    "                                   'lab_member_name' : your_name}, skip_duplicates=True)\n",
    "    print(f'Hi {your_name}! You have just been added to the LorenLab team. Congrats!')\n",
    "else:\n",
    "    print(f'Hi {your_name}! You are already on the team. Congrats!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting the NWB filename to be looked at\n",
    "NWB filenames take the form of an animal name plus the date of the recording.<br>For this tutorial, we will use the nwb file `'montague20200802_.nwb'`. The animal name is `'montague'` and the date of the recording is `'20200802'` the `'_'` indicates that this is a copy of the original NWB file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nwb_file_name = 'montague20200802_.nwb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can also be set programmatically by setting the `animal_name` and searching for a specified `date` in available NWB files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_name = 'montague'\n",
    "date = '20200802'\n",
    "nwb_files = (Session() & {'subject_id': animal_name}).fetch('nwb_file_name')\n",
    "nwb_file_name = [file for file in nwb_files if date in file][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting what part of a recording we want to sort\n",
    "### SortGroup()\n",
    "For each NWB file there will be multiple electrodes available to sort spikes from.<br>We commonly sort over multiple electrodes at a time, also referred to as a `SortGroup`.<br>This is accomplished by grouping electrodes according to what tetrode or shank of a probe they were on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sort group\n",
    "SortGroup().set_group_by_shank(nwb_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each electrode will have an `electrode_id` and be associated with an `electrode_group_name`, which will correspond with a `sort_group_id`. In this case, the data was recorded from a 32 tetrode (128 channel) drive, and thus results in 128 unique `electrode_id`, 32 unique `electrode_group_name`, and 32 unique `sort_group_id`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SortGroup.SortGroupElectrode & {'nwb_file_name': nwb_file_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sort_group_array = (SortGroup.SortGroupElectrode & {'nwb_file_name': nwb_file_name}).fetch('electrode_id',\n",
    "                                                                                           'electrode_group_name',\n",
    "                                                                                           'sort_group_id')\n",
    "print(f\"There are {len(np.unique(sort_group_array[0]))} unique electrode_id's, \\\n",
    "{len(np.unique(sort_group_array[1]))} unique electrode_group_name's, \\\n",
    "and {len(np.unique(sort_group_array[2]))} unique sort_group_id's\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### IntervalList()\n",
    "Next, we make a decision about the time interval for our spike sorting. Let's re-examine `IntervalList`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IntervalList & {'nwb_file_name' : nwb_file_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example, let's choose start with the first run interval (`02_r1`) as our sort interval. We first fetch `valid_times` for this interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_list_name = '02_r1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_list = (IntervalList & {'nwb_file_name' : nwb_file_name,\n",
    "                            'interval_list_name' : interval_list_name}).fetch1('valid_times')\n",
    "print(f'IntervalList begins as a {np.round((interval_list[0][1] - interval_list[0][0]) / 60,0):g} min long epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SortInterval()\n",
    "For brevity's sake, we'll select only the first 600 seconds of that 90 minute epoch as our sort interval. To do so, we first fetch `valid_times` of this interval, and then define our new sort interval as the first index of `interval_list` plus 600 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_interval = interval_list[0]\n",
    "sort_interval_name = interval_list_name + '_first600'\n",
    "sort_interval = np.copy(interval_list[0]) \n",
    "sort_interval[1] = sort_interval[0]+600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add this `sort_interval` with the specified `sort_interval_name` `'02_r1_first600'` to the `SortInterval` table. The `SortInterval.insert()` function requires the arguments input as a dictionary with keys `nwb_file_name`, `sort_interval_name`, and `sort_interval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SortInterval.insert1({'nwb_file_name' : nwb_file_name,\n",
    "                     'sort_interval_name' : sort_interval_name,\n",
    "                     'sort_interval' : sort_interval}, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've inserted the entry into `SortInterval()` you can see that entry by querying `SortInterval()` using the `nwb_file_name` and `sort_interval_name`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SortInterval & {'nwb_file_name' : nwb_file_name, 'sort_interval_name': sort_interval_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the `.fetch()` command, you can retrieve your user-defined sort interval from the `SortInterval` table.<br>A quick double-check will show that it is indeed a 600 second segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetched_sort_interval = (SortInterval & {'nwb_file_name' : nwb_file_name,\n",
    "                                      'sort_interval_name': sort_interval_name}).fetch('sort_interval')[0]\n",
    "print(f'The sort interval goes from {fetched_sort_interval[0]} to {fetched_sort_interval[1]}, \\\n",
    "which is {(fetched_sort_interval[1] - fetched_sort_interval[0])} seconds. COOL!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpikeSortingFilterParameters()\n",
    "Let's first take a look at the `SpikeSortingFilterParameters()` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeSortingFilterParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set the filtering parameters. Here we insert the default parameters, and then fetch the default parameter dictionary.<br>Note the lack of `[0]` after the `fetch1()` command compared to previous uses of `fetch()`, since it will only return a single object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeSortingFilterParameters().insert_default()\n",
    "filter_param_dict = (SpikeSortingFilterParameters() &\n",
    "                     {'filter_parameter_set_name': 'default'}).fetch1('filter_parameter_dict')\n",
    "print(f'{filter_param_dict}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the `frequency_min` parameter, and insert that into `SpikeSortingFilterParameters()` as a new set of filtering parameters for hippocampal data, named `'franklab_default_hippocampus'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_param_dict['frequency_min'] = 600\n",
    "SpikeSortingFilterParameters().insert1({'filter_parameter_set_name': 'franklab_default_hippocampus', \n",
    "                                       'filter_parameter_dict' : filter_param_dict}, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpikeSortingArtifactParameters()\n",
    "Similarly, we set up the `SpikeSortingArtifactParameters` which can allow us to remove artifacts from the data.<br>\n",
    "For the moment we just set up a `\"none\"` parameter set, which will do nothing when used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeSortingArtifactDetectionParameters().insert_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting a key\n",
    "Now we set up the parameters of the recording we are interested in, so we can get the recording extractor.<br>The `sort_group_id` refers back to the `SortGroup` table we populated at the beginning of the tutorial. We'll use `sort_group_id` 10 here. <br>Our `sort_interval_name` is the same as above: `'02_r1_first600'`.<br>Our `filter_param_name` and `artifact_param_name` are the same ones we just inserted into `SpikeSortingFilterParameters()` and `SpikeSortingArtifactDetectionParameters()`, respectively.<br>The `interval_list` was also set above as `'02_r1'`. Unlike `sort_interval_name`, which reflects our subsection of the recording, we keep `interval_list` unchanged from the original epoch name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_group_id = 10\n",
    "sort_interval_name = '02_r1_first600'\n",
    "filter_param_name = 'franklab_default_hippocampus'\n",
    "artifact_param_name = 'none'\n",
    "interval_list = '02_r1'\n",
    "lab_team = 'LorenLab'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make a dictionary to hold all these values, which will make querying and inserting into tables all the easier moving forward.<br>We'll assign this to `ssr_key` as these values are relvant to the recording we'll use to spike sort, also referred to as the spike sorting recording **(ssr)** :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = dict()\n",
    "key['nwb_file_name'] = nwb_file_name\n",
    "key['sort_group_id'] = sort_group_id\n",
    "key['sort_interval_name'] = sort_interval_name\n",
    "key['filter_parameter_set_name'] = filter_param_name\n",
    "key['artifact_parameter_name'] = artifact_param_name\n",
    "key['interval_list_name'] = interval_list\n",
    "key['team_name'] = lab_team\n",
    "\n",
    "ssr_key = key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpikeSortingRecordingSelection()\n",
    "We now insert all of these parameters into the `SpikeSortingRecordingSelection()` table, which we will use to specify what time/tetrode/etc of the recording we want to extract. By specifying all of those names in the previous cell, we're identifying which entries from the `SortGroup`, `SortInterval`, `SpikeSortingFilterParameters`, `IntervalList`, `SpikeSortingArtifactDetectionParameters`, and `LabTeam` tables we want to pass into `SpikeSortingRecordingSelection`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeSortingRecordingSelection.insert1(ssr_key, skip_duplicates=True)\n",
    "SpikeSortingRecordingSelection() & ssr_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpikeSortingRecording()\n",
    "And now we're ready to extract the recording! We use the `.proj()` command to pass along all of the primary keys from the `SpikeSortingRecordingSelection()` table to the `SpikeSortingRecording` table, so it knows exactly what to extract.<br>**Note**: we're using `ssr_key` to specify this exact set of parameters.<br>**Note 2**: This step might take a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeSortingRecording.populate([(SpikeSortingRecordingSelection & ssr_key).proj()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can see our recording in the table. _E x c i t i n g !_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeSortingRecording() & ssr_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpikeSortingWorkspace()\n",
    "Now we need to populate the `SpikeSortingWorkspace` table to make this recording available via kachery (our server backend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SpikeSortingWorkspace.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeSortingWorkspace() & ssr_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of an aside... you can now access the workspace using the `sortingview` package. Uncomment and run the cell below if you want to explore a bit.<br>The workspace is an object that contains the recording object and eventually the sorting object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sortingview as sv\n",
    "# workspace = sv.load_workspace((SpikeSortingWorkspace() & ssr_key).fetch1('workspace_uri'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpikeSorter() setup\n",
    "For our example, we will be using `mountainsort4`. There are already some default parameters in the `SpikeSorterParameters()` table we'll `fetch`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SpikeSorter().insert_from_spikeinterface()\n",
    "SpikeSorterParameters().insert_from_spikeinterface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the default params\n",
    "sorter_name='mountainsort4'\n",
    "ms4_default_params = (SpikeSorterParameters & {'sorter_name' : sorter_name,\n",
    "                                               'spikesorter_parameter_set_name' : 'default'}).fetch1()\n",
    "print(ms4_default_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can change these default parameters to line up more closely with our preferences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_dict = ms4_default_params['parameter_dict']\n",
    "# Detect downward going spikes (1 is for upward, 0 is for both up and down)\n",
    "param_dict['detect_sign'] = -1 \n",
    "# We will sort electrodes together that are within 100 microns of each other\n",
    "param_dict['adjacency_radius'] = 100\n",
    "param_dict['curation'] = False\n",
    "# Turn filter off since we will filter it prior to starting sort\n",
    "param_dict['filter'] = False\n",
    "param_dict['freq_min'] = 0\n",
    "param_dict['freq_max'] = 0\n",
    "# Turn whiten off since we will whiten it prior to starting sort\n",
    "param_dict['whiten'] = False\n",
    "# set num_workers to be the same number as the number of electrodes\n",
    "param_dict['num_workers'] = 4\n",
    "param_dict['verbose'] = True\n",
    "# set clip size as number of samples for 1.33 millisecond based on the sampling rate\n",
    "param_dict['clip_size'] = np.int(1.33e-3 * (Raw & {'nwb_file_name' : nwb_file_name}).fetch1('sampling_rate'))\n",
    "param_dict['noise_overlap_threshold'] = 0\n",
    "param_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This set of parameters has already been inserted into the table as `'franklab_tetrode_hippocampus_30KHz'`.<br>We can take a look at these and insert a new `spikesorter_parameter_set_name` and `parameter_dict` into the `SpikeSorterParameters()` table if need be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeSorterParameters() & {'sorter_name' : sorter_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(SpikeSorterParameters() & {'sorter_name' : sorter_name,\n",
    "                           'spikesorter_parameter_set_name' : 'franklab_tetrode_hippocampus_30KHz'}).fetch1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Give a unique name here if parameters different than default\n",
    "parameter_set_name = 'franklab_tetrode_hippocampus_30KHz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we insert our parameters for use by the spike sorter into `SpikeSorterParameters()` and double-check that it made it in to the table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SpikeSorterParameters.insert1({'sorter_name': sorter_name,\n",
    "                               'spikesorter_parameter_set_name': parameter_set_name,\n",
    "                               'parameter_dict': param_dict}, skip_duplicates=True)\n",
    "# Check that insert was successful\n",
    "p = (SpikeSorterParameters & {'sorter_name': sorter_name, 'spikesorter_parameter_set_name': parameter_set_name}).fetch1()\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gearing up to Spike Sort by adding to `SpikeSortingSelection()`\n",
    "\n",
    "We now collect all the decisions we made up to here and put it into the `SpikeSortingSelection` table, which is specific to this recording and eventual sorting segment.<br>We'll add in a few parameters to our key and call it `ss_key` for spike sorting key now.<br>(**note**: the spike *sorter* parameters defined above are for the sorter, `mountainsort4` in this case.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = (SpikeSortingWorkspace & ssr_key).fetch1(\"KEY\")\n",
    "key['sorter_name'] = sorter_name\n",
    "key['spikesorter_parameter_set_name'] = 'franklab_tetrode_hippocampus_30KHz'\n",
    "ss_key = key\n",
    "SpikeSortingSelection.insert1(ss_key, skip_duplicates=True)\n",
    "(SpikeSortingSelection & ss_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Spike Sorting\n",
    "Now we can run spike sorting. It's nothing more than populating a table (`SpikeSorting`) based on the entries of `SpikeSortingSelection`.<br>**Note**: This will take a little bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# `proj` gives you primary key\"\n",
    "SpikeSorting.populate([(SpikeSortingSelection & ss_key).proj()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check to make sure the table populated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeSorting() & ss_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpikeSortingMetricParameters()\n",
    "#### Define quality metric parameters for curation with `SpikeSortingMetricParameters()` table\n",
    "\n",
    "We're almost done. There are more parameters related to how to compute the quality metrics for curation. We just use the default options here. The default has already been inserted into the table as `'franklab_cluster_metrics_09-19-2021'`.<br>For this tutorial we'll go through the motions of adding it, regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeSortingMetricParameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we'll take a look at what the default set of metrics are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict = SpikeSortingMetricParameters().get_metric_dict()\n",
    "metric_param_dict = SpikeSortingMetricParameters().get_metric_parameter_dict()\n",
    "for k in metric_dict:\n",
    "    print(f\"'{k}': {metric_dict[k]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now set the ones we want to calculate to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_dict['noise_overlap'] = True\n",
    "metric_dict['firing_rate'] = True\n",
    "metric_dict['num_spikes'] = True\n",
    "for k in metric_dict:\n",
    "    print(f\"'{k}': {metric_dict[k]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_metrics_list_name = 'franklab_cluster_metrics_09-19-2021'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And now add the cluster metrics to the `SpikeSortingMetricParameters()` table.\n",
    "**Note** we have `skip_duplicates=True`, so if an entry with the same name already exists in the table, a new one won't get inserted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SpikeSortingMetricParameters.insert1({'cluster_metrics_list_name' : cluster_metrics_list_name,\n",
    "                            'metric_dict' : metric_dict, \n",
    "                            'metric_parameter_dict' : metric_param_dict}, skip_duplicates=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Curation: AutomaticCurationParameters(), AutomaticCurationSelection()\n",
    "#### Retrieve the default automatic curation parameters and add to `AutomaticCurationParameters()` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = AutomaticCurationParameters().get_default_parameters()\n",
    "AutomaticCurationParameters().insert1({'automatic_curation_parameter_set_name':'none', \n",
    "                                      'automatic_curation_parameter_dict': param}, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add an entry to `AutomaticCurationSelection()` to select those parameters for automatic curation of this sorting.\n",
    "First we'll get the sorting-id from the `SpikeSorting` table. And then identify which entries from `AutomaticCurationParameters()` and `SpikeSortingMetricParameters()` we want to use during automatic curation.<br>This will all get added into a new automatic curation selection key (`acs_key`).<br>**Note** This is similar to how we added parameters to `SpikeSortingSelection()` prior to populating `SpikeSorting()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acs_key = (SpikeSortingRecording & ssr_key).fetch1('KEY')\n",
    "acs_key['sorting_id'] = (SpikeSorting & ss_key).fetch1('sorting_id')\n",
    "acs_key['automatic_curation_parameter_set_name'] = 'none'\n",
    "acs_key['cluster_metrics_list_name'] = cluster_metrics_list_name\n",
    "AutomaticCurationSelection.insert1(acs_key, skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(AutomaticCurationSelection() & acs_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll stop here for now... there are some bugs below that need to be worked out :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we populate the `AutomaticCuration()` table, which in this case just computes the metrics and does not add labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutomaticCuration.populate(acs_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutomaticCuration() & acs_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisiting `SpikeSortingWorkspace()`.\n",
    "To peform manual curation, we use the `figurl` interface.<br>`figurl` will load more quickly if we run `SpikeSortingWorkspace().precalculate()` beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpikeSortingWorkspace().precalculate(ssr_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll use the sortingview backend to access the figurl *url*. We do this by loading the workspace that we set up while populating `SpikeSortingWorkspace`.<br>This workspace in tandem with the ids of the spike sorting and recording segment we extracted during this tutorial, will allow us to retrieve the url. We'll also use this opportunity to enable permissions for everyone to curate this sorting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sortingview as sv\n",
    "workspace = sv.load_workspace((SpikeSortingWorkspace() & ssr_key).fetch1('workspace_uri'))\n",
    "sorting_id = acs_key['sorting_id']\n",
    "recording_id = workspace.recording_ids[0]\n",
    "url = workspace.experimental_spikesortingview(recording_id=recording_id, sorting_id=sorting_id,\n",
    "                                                  label=workspace.label, include_curation=True)\n",
    "member_emails = LabMember().LabMemberInfo().fetch('lab_member_name','google_user_name')\n",
    "member_dict = [email for name, email in zip(member_emails[0], member_emails[1]) if name in lorenlab_team_members]\n",
    "workspace.set_sorting_curation_authorized_users(sorting_id=sorting_id, user_ids=member_dict)\n",
    "print(f'{url}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're done with manual curation through figurl, you can click 'Close Curation' or uncomment and run the cell below to close your curation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workspace.add_sorting_curation_action(sorting_id=sorting_id, action={'type':'CLOSE_CURATION'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CuratedSpikeSorting()\n",
    "Now you can add the units (with the option for a new set of metrics) to the `CuratedSpikeSorting` table, which includes only accepted units.<br>This is accomplished by first adding an entry to `CuratedSpikeSortingSelection()` and then populating `CuratedSpikeSorting` from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "css_key = (AutomaticCuration & acs_key).fetch1('KEY')\n",
    "css_key['sorting_id']\n",
    "css_key['final_cluster_metrics_list_name'] = cluster_metrics_list_name\n",
    "CuratedSpikeSortingSelection.insert1(css_key, skip_duplicates=True)\n",
    "CuratedSpikeSorting.populate(css_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now you can see all your accepted units in the `Unit` table with `CuratedSpikeSorting`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CuratedSpikeSorting().Unit() & css_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nwb_datajoint] *",
   "language": "python",
   "name": "conda-env-nwb_datajoint-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
